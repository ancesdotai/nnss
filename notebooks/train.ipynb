{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 0\n",
    "\n",
    "!pip3 install numpy\n",
    "!pip3 install matplotlib\n",
    "!pip3 install tensorflow\n",
    "!pip3 install mido\n",
    "!pip3 install tqdm\n",
    "\n",
    "#if data/piano_scale.wav does not exist, download it from https://ances.ai/nnss/piano_scale.wav with python requests\n",
    "import requests\n",
    "import os\n",
    "if not os.path.exists('data/piano_scale.wav'):\n",
    "    r = requests.get('http://ances.ai/nnss/piano_scale.wav')\n",
    "    with open('data/piano_scale.wav', 'wb') as f:\n",
    "        f.write(r.content)\n",
    "print('done')     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 1\n",
    "\n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import IPython.display as ipd\n",
    "import soundfile as sf\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD, schedules\n",
    "from tensorflow.keras.callbacks import LambdaCallback, ModelCheckpoint\n",
    "import json\n",
    "import os\n",
    "import mido\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "%matplotlib notebook\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [9, 7]\n",
    "\n",
    "tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "def to_np(tensor):\n",
    "    return tensor.eval(session=tf.compat.v1.Session())\n",
    "\n",
    "tf.compat.v1.experimental.output_all_intermediates(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of functions we need and global variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 2\n",
    "\n",
    "def get_notes(midifile):\n",
    "    mid = mido.MidiFile(midifile, clip=True)\n",
    "    ticks_per_beat = mid.ticks_per_beat\n",
    "\n",
    "    for note in mid.tracks[0]:\n",
    "        if note.type == \"set_tempo\":\n",
    "            tempo = note.tempo\n",
    "            \n",
    "    time_cum = 0\n",
    "    notes=[]\n",
    "\n",
    "    for note in mid.tracks[1]:\n",
    "        time_cum += note.time\n",
    "        if note.type == \"note_on\":\n",
    "            t = mido.tick2second(time_cum, ticks_per_beat, tempo)\n",
    "            notes.append({'note':note.note,'velocity':note.velocity,'start':t})\n",
    "        elif note.type==\"note_off\":\n",
    "            t = mido.tick2second(time_cum, ticks_per_beat, tempo)\n",
    "            for a in range(0,len(notes)):\n",
    "                if notes[a]['note']==note.note and 'end' not in notes[a]:\n",
    "                    notes[a]['end']=t\n",
    "    return notes\n",
    "\n",
    "def merge(chunks,offset):\n",
    "    audio=chunks[0]\n",
    "    for a in range(1,len(chunks)):\n",
    "        audio[-2*offset:] += chunks[a][:2*offset]\n",
    "        audio=np.concatenate((audio,chunks[a][2*offset:]))\n",
    "    return audio\n",
    "\n",
    "def ances_normalization(s,power=1):\n",
    "    ab = np.power(np.abs(s),power)\n",
    "    an = np.angle(s)\n",
    "    return ab*np.exp(1j*an) \n",
    "\n",
    "\n",
    "def generate_files(audio, file_prefix=\"X\", shuffle=True):\n",
    "    len_chunk=FRAME_LENGTH+(FRAME_STEP*(STEPS-1))\n",
    "    #interpolate audio\n",
    "    audio = np.interp(np.arange(0, len(audio), 1/FACTOR), np.arange(0, len(audio)), audio)\n",
    "    if shuffle:\n",
    "        np.random.seed(0)\n",
    "        permutation = np.random.permutation((audio.shape[0]-len_chunk)//(STEPS*FRAME_STEP))*STEPS*FRAME_STEP\n",
    "    else:\n",
    "        permutation = np.arange(0,(audio.shape[0]-len_chunk)//(STEPS*FRAME_STEP))*STEPS*FRAME_STEP\n",
    "\n",
    "    arr = []\n",
    "    part=0\n",
    "    end=MAX_FILES*BATCH_LIMIT\n",
    "    for a in tqdm(permutation[:end]):\n",
    "        chunk=audio[a:a+len_chunk]\n",
    "        if chunk.shape[0]<(len_chunk):\n",
    "            break\n",
    "        else:\n",
    "            spectrogram=tf.signal.stft(chunk,frame_length=FRAME_LENGTH,frame_step=FRAME_STEP, window_fn=tf.signal.hann_window).numpy().astype(np.complex64)\n",
    "            spectrogram=spectrogram[:,:BINS_LIMIT]\n",
    "            arr.append(spectrogram) \n",
    "        if len(arr)>=BATCH_LIMIT:\n",
    "            arr=ances_normalization(np.array(arr),1/POWER_RATE) \n",
    "            pickle.dump(arr, open(DATA_FOLDER+file_prefix+'{:04d}'.format(part)+\".data\", \"wb\"), protocol=4)\n",
    "            arr=[]\n",
    "            part+=1\n",
    "\n",
    "def encode_notes(notes):\n",
    "    len_encode = int(max([note['end'] for note in notes]))\n",
    "    encoding=np.zeros((len_encode*SAMPLERATE+SAMPLERATE))\n",
    "    _m1=np.arange(0.,SAMPLERATE*5,1.)/(SAMPLERATE*5)\n",
    "    for note in tqdm(notes):\n",
    "        start=int(note['start']*SAMPLERATE)\n",
    "        end=int(note['end']*SAMPLERATE)\n",
    "        s=np.zeros((end-start,))\n",
    "        time=np.arange(0.,end-start,1.)\n",
    "\n",
    "        s+=1.1*np.sin(np.pi*time*(1-time/(SAMPLERATE*5)))*1.1\n",
    "        s+=1.1*np.sin(np.pi*time*(note['note'])/(127*1.5))*1.1\n",
    "        encoding[start:end]+=s\n",
    "        \n",
    "    encoding[encoding>1]=1 #to add some harmonics\n",
    "    return encoding\n",
    "\n",
    "\n",
    "FACTOR=8\n",
    "FRAME_STEP=1\n",
    "FRAME_LENGTH=2048*FACTOR\n",
    "STEPS=400\n",
    "BATCH_LIMIT=640\n",
    "BINS_LIMIT=1024\n",
    "MAX_FILES=400\n",
    "POWER_RATE=8\n",
    "SAMPLERATE=44100\n",
    "DATA_FOLDER=\"2TB/synt/\"\n",
    "BATCH_SIZE=64\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate spectrograms, run the code below. This could need a lot of HD memory. 2TB in my case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cell 3\n",
    "\n",
    "bach_notes=get_notes('data/bach_test_short.mid')\n",
    "escala_notes=get_notes('data/scale.mid')\n",
    "\n",
    "with open('data/piano_scale.wav', 'rb') as f:\n",
    "    piano, sr = sf.read(f)\n",
    "\n",
    "print('Encoding scale') \n",
    "encoding=encode_notes(escala_notes)[:piano.shape[0]]\n",
    "\n",
    "print('Encoding Bach test')\n",
    "encoding_test=encode_notes(bach_notes)\n",
    "\n",
    "#cleaning folder\n",
    "for filename in os.listdir(DATA_FOLDER):\n",
    "    if \"X_\" in filename or \"Y_\" in filename:\n",
    "        os.remove(DATA_FOLDER+filename)\n",
    "\n",
    "print(\"Generating X_test files...\")\n",
    "generate_files(encoding_test, file_prefix=\"X_test_\", shuffle=False)\n",
    "print(\"Generating X_ files...\")\n",
    "generate_files(encoding, file_prefix=\"X_\", shuffle=True)\n",
    "print(\"Generating Y_ files...\")\n",
    "generate_files(piano, file_prefix=\"Y_\", shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(DATA_FOLDER+\"X_test_0001.data\", \"rb\") as file:\n",
    "    X_test = pickle.load(file)\n",
    "\n",
    "image = np.abs(X_test[15]).squeeze().T\n",
    "plt.imshow(image,origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is generated, i have to reboot the kernel to free memory, and run the first and second cell again, jumping to cell 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 4\n",
    "\n",
    "def train_generator(batch_size=4):\n",
    "    files = [f.replace('.data', '').replace('Y_', '') for f in os.listdir(DATA_FOLDER) if f.endswith('.data') and f.startswith('Y_')]\n",
    "    files.sort()\n",
    "    while True:\n",
    "        for p in files[:-1]:\n",
    "            with open(DATA_FOLDER+\"X_\"+p+\".data\", \"rb\") as file:\n",
    "                X = pickle.load(file)\n",
    "            with open(DATA_FOLDER+\"Y_\"+p+\".data\", \"rb\") as file:\n",
    "                Y = pickle.load(file)\n",
    "            for offset in range(0, X.shape[0], batch_size):\n",
    "                yield X[offset:offset+batch_size], Y[offset:offset+batch_size]\n",
    "                \n",
    "def test_generator(batch_size=4):\n",
    "    files = [f.replace('.data', '').replace('Y_', '') for f in os.listdir(DATA_FOLDER) if f.endswith('.data') and f.startswith('Y_')]\n",
    "    files.sort()\n",
    "    while True:\n",
    "        with open(DATA_FOLDER+\"X_\"+files[-1]+\".data\", \"rb\") as file:\n",
    "            X = pickle.load(file)\n",
    "        with open(DATA_FOLDER+\"Y_\"+files[-1]+\".data\", \"rb\") as file:\n",
    "            Y = pickle.load(file)\n",
    "        for offset in range(0, X.shape[0], batch_size):\n",
    "            yield X[offset:offset+batch_size], Y[offset:offset+batch_size]\n",
    "\n",
    "#I know, it's slow as hell and inefficient. I'll fix it later. Calculate STFT and ISTFT consumes a lot of GPU memory\n",
    "def testor(batch_size=1,frame_step=1):\n",
    "    files = [f for f in os.listdir(DATA_FOLDER) if f.endswith('.data') and f.startswith('X_test_')]\n",
    "    files.sort()\n",
    "    rec=[]\n",
    "    for file in files:\n",
    "        with open(DATA_FOLDER+file, \"rb\") as file:\n",
    "            X_test = pickle.load(file)\n",
    "            \n",
    "        for offset in range(0, X_test.shape[0], batch_size):\n",
    "            pred = model.predict_on_batch(X_test[offset:offset+batch_size])\n",
    "            pred = ances_normalization(pred,8)\n",
    "            pred = tf.signal.inverse_stft(pred, frame_length=FRAME_LENGTH, \n",
    "                                                  frame_step=FRAME_STEP,\n",
    "                                                  window_fn=tf.signal.inverse_stft_window_fn(FRAME_STEP, forward_window_fn=tf.signal.hann_window)).numpy()\n",
    "            for r in pred:\n",
    "                rec.append(r)\n",
    "                \n",
    "    rec = np.array(rec)\n",
    "    return merge(rec.copy(),int(FRAME_LENGTH//2))\n",
    "\n",
    "#here the model\n",
    "\n",
    "in1 = Input(shape=(None,1024), dtype=tf.complex64)\n",
    "re = tf.math.real(in1)\n",
    "im = tf.math.imag(in1)\n",
    "co = Concatenate(axis=-1)([re,im])\n",
    "lstm = Bidirectional(LSTM(1000, return_sequences=True))(co)\n",
    "lstm = Bidirectional(LSTM(1000, return_sequences=True))(lstm)\n",
    "re = Dense(1024, activation=\"linear\", name=\"real\")(lstm)\n",
    "im = Dense(1024, activation=\"linear\", name=\"imag\")(lstm)\n",
    "out = tf.complex(re, im)\n",
    "\n",
    "model = Model(inputs=in1, outputs=out)\n",
    "\n",
    "opt = Adam(learning_rate=0.0002)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt) \n",
    "\n",
    "model.summary()\n",
    "\n",
    "#model.load_weights(\"weights/d00000097.h5\", by_name=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cell 5\n",
    "\n",
    "data_epoch = \"train0.txt\"\n",
    "\n",
    "#if os.path.exists(data_epoch):\n",
    "#    os.remove(data_epoch)\n",
    "\n",
    "def on_epoch_end(epoch, logs=None):\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Rendering the test\")\n",
    "        pred = testor()\n",
    "        pickle.dump(pred, open(\"audio_test/\"+str(epoch)+\".data\", \"wb\"), protocol=4)\n",
    "\n",
    "    with open(data_epoch, 'a') as the_file:\n",
    "        the_file.write(json.dumps(str(logs))+'\\n')\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "mc = ModelCheckpoint('weights/d{epoch:08d}.h5',\n",
    "                     save_weights_only=True, save_freq=200)\n",
    "\n",
    "history = model.fit(train_generator(batch_size=BATCH_SIZE),\n",
    "                    validation_data=test_generator(batch_size=BATCH_SIZE),\n",
    "                    steps_per_epoch=(MAX_FILES-1)*BATCH_LIMIT//BATCH_SIZE,\n",
    "                    validation_steps=BATCH_LIMIT//BATCH_SIZE,\n",
    "                    epochs=1000,\n",
    "                    initial_epoch=100,\n",
    "                    callbacks=[print_callback, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_FOLDER+\"X_0001.data\", \"rb\") as file:\n",
    "    X = pickle.load(file)\n",
    "    \n",
    "step = 64\n",
    "\n",
    "pred = model.predict_on_batch(X[:64])\n",
    "\n",
    "image = np.abs(pred[0]).squeeze().T\n",
    "plt.imshow(image,origin='lower')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "592.969px",
    "left": "764.875px",
    "right": "20px",
    "top": "76.9219px",
    "width": "443.875px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
